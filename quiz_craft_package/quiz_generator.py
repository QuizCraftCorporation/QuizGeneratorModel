import logging
from langchain.chat_models import ChatOpenAI
from .models.MCQ_generator import MCQGenerator
from .models.Boolean_generator import BooleanQGenerator
from .models.OpenEnded_generator import OpenQGenerator
from .utils.text_splitter import TextSplitter
from .utils.file_reader import FileReader
import time
import random

class QuizGenerator():

    def __init__(self, q_fraction: list[float] = [0.7, 0.2, 0.1], debug=False, delay: float = 10) -> None:
        """
        Loads all models and intialize splitter.

        Args:
            q_fraction (list[float]): The portion of each question types that should be generated by model.
            debug (bool): If true then output debug information.
            delay (float): Delay between requests to Open AI API
        """
        if len(q_fraction) != 3:
            raise Exception("Length of 'q_fraction' list must be equal to 3!")
        if sum(q_fraction) != 1:
            raise Exception("Sum of all elements of 'q_fraction' must be equal to 1")
        self.q_fraction = q_fraction
        self.delay = delay

        if debug:    
            logging.basicConfig(level=logging.INFO)

        logging.info("Loading models into RAM...")
        OPEN_AI_KEY = "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" 
        language_model = ChatOpenAI(openai_api_key=OPEN_AI_KEY, temperature=0, model="gpt-3.5-turbo")      
        self.MCQ_model = MCQGenerator(language_model)
        self.BooleanQ_model = BooleanQGenerator(language_model)
        self.OpenQ_model = OpenQGenerator(language_model)
        
        logging.info("Models loaded succesfully")
        
        self.text_splitter = TextSplitter()



    def create_questions_from_files(self, file_paths: list[str]) -> list:
        """
        Create questions from specified files.

        Args:
            file_paths (list[str]): The index of the video to retrieve.
        Returns:
            list: A list that contains all generated question. Format of questions is decribed in README.md.
        """
        complex_quiz = []
        for file_path in file_paths:
            logging.info(f"PROCESSING FILE {file_path}")
            text_data = FileReader(file_path).get_content()
            complex_quiz += self.create_questions(text_data)
        return complex_quiz

    def create_questions(self, text: str):
        """
        Create questions from plain text.
        """
        
        logging.info("Splitting text")
        text_chunks = self.text_splitter.split_text(text)
        logging.info(f"Text splitted into {len(text_chunks)} chunks")
        question_types_arrays = []
        for i in range(0, len(self.q_fraction)):
            question_types_arrays += [i]*int(self.q_fraction[i]*len(text_chunks))
        question_types_arrays += [0]*(len(text_chunks)-len(question_types_arrays))
        logging.info(f"MCQ:{question_types_arrays.count(0)}, Boolean:{question_types_arrays.count(1)}, Open:{question_types_arrays.count(2)}")
        random.shuffle(question_types_arrays)

        result = []
        logging.info(f"Start processing text chunks. Number of chunks is {len(text_chunks)}.")
        for i in range(len(text_chunks)):
            questions = []
            if question_types_arrays[i] == 0:
                questions = self.MCQ_model.generate_question(text_chunks[i])
            elif question_types_arrays[i] == 1:
                questions = self.BooleanQ_model.generate_question(text_chunks[i])
            elif question_types_arrays[i] == 2:
                questions = self.OpenQ_model.generate_question(text_chunks[i])
            
            logging.info(f"{i+1} of {len(text_chunks)} chunks scanned!")
            logging.info(f"Add {len(questions)} new questions!")
            result += questions
            time.sleep(self.delay)

        return result
