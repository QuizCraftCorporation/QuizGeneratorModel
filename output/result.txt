Thompson Sampling for the Bernoulli Bandit
To incorporate a Bayesian model to represent uncertainty in a Bernoulli bandit|To test the effectiveness of Thompson Sampling in a real-world scenario|To compare the performance of Thompson Sampling with other bandit algorithms|To analyze the distribution of rewards in a Bernoulli bandit
[0]


The Beta-Bernoulli Bandit
A bandit with K actions that produces a reward of one with probability θk and a reward of zero with probability 1-θk|A bandit with K actions that produces a reward of one with probability 1-θk and a reward of zero with probability θk|A bandit with K actions that produces a reward of one with probability θk and a reward of two with probability 1-θk|A bandit with K actions that produces a reward of one with probability 1-θk and a reward of two with probability θk
[0]


The Prior Belief of the Agent
The agent has no prior belief over each θk|The agent's prior belief over each θk is beta-distributed with parameters α and β|The agent's prior belief over each θk is normally distributed with parameters μ and σ|The agent's prior belief over each θk is uniform
[1]


Updating the Distribution
The distribution is updated according to Thompson Sampling|The distribution is updated according to Bayes' rule|The distribution is updated according to the maximum likelihood estimate|The distribution is updated according to the mean of the rewards
[1]


What is the advantage of using beta distributions in updating the posterior distribution in a bandit problem?
They have conjugacy properties|They are easier to work with than other distributions|They have a uniform prior distribution|They have a higher mean than other distributions
[0]


What happens to the parameters of a selected action in a beta distribution when there is an observed success or failure?
αk increases with success and βk increases with failure|αk increases with failure and βk increases with success|Both αk and βk increase with success|Both αk and βk increase with failure
[0]


What is the meaning of pseudo-counts in a beta distribution?
The number of observed successes or failures|The number of times an action has been selected|The number of times a reward has been observed|The number of times a distribution has been updated
[0]


What is the mean of a beta distribution with parameters (αk,βk)?
αk/(αk+βk)|(αk+βk)/2|αk*βk|1/(αk+βk)
[0]


What happens to the concentration of a beta distribution as αk+βk grows?
It becomes more concentrated|It becomes less concentrated|It remains the same|It becomes uniform
[0]


What is the main idea behind the greedy algorithm for the beta-Bernoulli bandit?
Select the action with the largest estimate of success probability|Select the action with the smallest estimate of success probability|Select a random action|Select the action with the highest reward in the previous time period
[0]


What is the estimate used by the greedy algorithm for the success probability of an action?
αk/(αk+βk)|βk/(αk+βk)|αk+βk|1/(αk+βk)
[0]


What happens to the distribution parameters in the beta-Bernoulli bandit after an action is selected and a reward is observed?
They are updated|They remain the same|They are reset to their initial values|They are randomly changed
[0]


What is the purpose of TS in the context of the Bernoulli bandit problem?
To explore to resolve uncertainty and identify the optimal action while avoiding probing where feedback would not be helpful.
[0]


What is the difference between a greedy action and a TS action in the Bernoulli bandit problem?
A greedy action would forgo the potentially valuable opportunity to learn about certain actions, while TS explores to resolve uncertainty where there is a chance that resolution will help the agent identify the optimal action, but avoids probing where feedback would not be helpful.
[0]


In the simulated behavior comparison between TS and a greedy algorithm, what were the mean rewards for each arm of the three-armed beta-Bernoulli bandit?
θ1= 0.9, θ2= 0.8, and θ3= 0.7.
[0]


How many independent simulations were conducted for each algorithm in the simulated behavior comparison between TS and a greedy algorithm?
Ten thousand.
[0]


What is the purpose of randomly rank-ordering actions in each simulation?
To avoid bias towards selecting any particular action|To ensure that the optimal action is always selected|To make the algorithm converge faster|To increase the expected mean reward
[0]


Why does the greedy algorithm sometimes fail to converge on the optimal action?
Because it can get stuck repeatedly applying a poor action|Because it is biased towards selecting a particular action|Because it does not have enough data to make an informed decision|Because it is too complex to implement
[0]


How does Thompson Sampling perform compared to the greedy algorithm?
It learns to select the optimal action within the thousand periods|It gets stuck repeatedly applying a poor action|It is biased towards selecting a particular action|It has a lower expected mean reward
[0]


What is the per-period regret of an algorithm over a time period t?
The difference between the mean reward of an optimal action and the action selected by the algorithm|The total number of simulations run by the algorithm|The expected mean reward of the algorithm|The probability that the algorithm selects the optimal action
[0]


What is the purpose of Figure 3.2a?
To plot per-period regret realized by the greedy algorithm and TS|To compare algorithms with fixed mean rewards|To examine regret averaged over plausible values of θ|To sample many instances of θ from the prior distributions
[0]


What happens to the average per-period regret of TS as time progresses?
It vanishes|It increases|It stays the same|It decreases
[0]


Why is the per-period regret of randomly selected actions smaller in early time periods in Figure 3.2a?
Because with θ= (0.9,0.8,0.7), mean rewards are closer than for a typical randomly sampled θ|Because the rewards are larger in early time periods|Because the rewards are smaller in early time periods|Because the algorithm is more efficient in early time periods
[0]


Why is the per-period regret of TS larger in Figure 3.2a than Figure 3.2b over later time periods?
Because of proximity among rewards with θ= (0.9,0.8,0.7)|Because of proximity among rewards with θ= (0.1,0.2,0.3)|Because of proximity among rewards with θ= (0.5,0.5,0.5)|Because of proximity among rewards with θ= (1,1,1)
[0]


What is the purpose of Figure 3.2b?
To plot averages over ten thousand simulations with each action reward sampled independently from a uniform prior for each simulation|To compare algorithms with fixed mean rewards|To examine regret averaged over plausible values of θ|To sample many instances of θ from the prior distributions
[0]


What is the General Thompson Sampling?
A method to apply to a broad array of online decision problems|A method to apply only to the Bernoulli bandit|A method to apply to offline decision problems|A method to apply to a broad array of offline decision problems
[0]


What is the difference between the greedy algorithm and the TS approach?
The way they generate model parameters ˆθ|The way they select actions|The way they update the prior distribution p|The way they calculate the expected reward
[0]


How does the greedy algorithm generate ˆθ?
It takes ˆθ to be the expectation of θ with respect to the distribution p|It draws a random sample from p|It calculates the maximum likelihood estimate of θ|It uses a Bayesian approach to update the prior distribution p
[0]


How does the TS approach generate ˆθ?
It draws a random sample from p|It takes ˆθ to be the expectation of θ with respect to the distribution p|It calculates the maximum likelihood estimate of θ|It uses a Bayesian approach to update the prior distribution p
[0]


What is the reward function in the general setting?
r(yt)|qθ(·|xt)|p|ˆθ
[0]


How does the agent represent their uncertainty about θ?
Using a prior distribution p|Using a posterior distribution p|Using a likelihood function|Using a maximum likelihood estimate
[0]


What is the purpose of Algorithm 3 and Algorithm 4 in the given text?
To estimate the model and update the distribution|To select and apply action|To sample the model and observe the reward|To model the conditional probabilities
[0, 0, 0, 0]


What is the formula for the conditional distribution in Equation (4.2)?
Pp,q(θ=u|xt,yt) =p(u)qu(yt|xt)/summationtextvp(v)qv(yt|xt)|Pp,q(θ=u|xt,yt) =p(u)qv(yt|xt)/summationtextvp(v)qv(yt|xt)|Pp,q(θ=u|xt,yt) =q(u)qu(yt|xt)/summationtextvp(v)qv(yt|xt)|Pp,q(θ=u|xt,yt) =q(u)qv(yt|xt)/summationtextvp(v)qv(yt|xt)
[0, 0, 0, 0]


What is the special case of the more general formulation discussed in the given text?
Bernoulli bandit with a beta prior|Gaussian bandit with a normal prior|Poisson bandit with a gamma prior|Exponential bandit with an exponential prior
[0, 0, 0, 0]


What is the prior distribution for the model's parameters?
Beta distribution|Gamma distribution|Normal distribution|Poisson distribution
[0]


What is the difference between the greedy algorithm and TS in this context?
The greedy algorithm sets ˆθk to the expected value of θk, while TS randomly draws ˆθk from a beta distribution with parameters (αk,βk)|The greedy algorithm randomly draws ˆθk from a beta distribution with parameters (αk,βk), while TS sets ˆθk to the expected value of θk|The greedy algorithm and TS both set ˆθk to the expected value of θk|The greedy algorithm and TS both randomly draw ˆθk from a beta distribution with parameters (αk,βk)
[0]


How are the belief distribution parameters updated after an action is taken?
(α,β)←(α+rt1xt,β+ (1−rt)1xt)|(α,β)←(α+rt,β+ (1−rt))|(α,β)←(α+rt,β+rt)|(α,β)←(α+rt1xt,β+rt1xt)
[0]


Can the algorithms presented in this text be applied to complex problems?
Yes|No
[0]


What is the problem presented in Example 1.2?
A shortest path problem|A scheduling problem|A machine learning problem|A data visualization problem
[0]


What is the meaning of θ in the problem?
The set of vertices in the graph|The set of edges in the graph|The mean travel times of the edges in the graph|The source and destination vertices in the graph
[2]


What is the meaning of xt in the problem?
The set of edges leading from source to destination|The set of all possible paths in the graph|The set of all vertices in the graph|The set of all edges in the graph
[0]


What is the meaning of yt,e in the problem?
The travel time of edge e in action xt|The cost incurred by the agent for traversing edge e|The reward obtained by the agent for traversing edge e|The prior distribution of θe
[0]


What is the prior distribution of θe in the problem?
Log-Gaussian distribution|Uniform distribution|Exponential distribution|Poisson distribution
[0]


What is the meaning of µe in the problem?
The mean of the log-Gaussian distribution of θe|The variance of the log-Gaussian distribution of θe|The mean travel time of edge e|The cost incurred by the agent for traversing edge e
[0]


What is the meaning of σ2e in the problem?
The variance of the log-Gaussian distribution of θe|The mean of the log-Gaussian distribution of θe|The mean travel time of edge e|The cost incurred by the agent for traversing edge e
[0]


What is the meaning of ˜σ2 in the problem?
The variance of the log-Gaussian distribution of yt,e|The mean of the log-Gaussian distribution of yt,e|The mean travel time of edge e|The cost incurred by the agent for traversing edge e
[0]


What is the rule for updating the distribution of θe upon observation of yt,e?
(µe,σ2e)←1/σ2eµe+1/˜σ2|(µe,σ2e)←1/˜σ2µe+1/σ2e|(µe,σ2e)←σ2eµe+˜σ2/2|(µe,σ2e)←˜σ2µe+σ2e/2
[0]


What is the motivation behind the log-Gaussian prior in this context?
To construct a prior for which expectations are equal to travel distances|To express a degree of uncertainty in the travel times|To maximize the expected value of θe|To minimize the variance of mean travel time along an edge
[0]


What is the prior variance of mean travel time along an edge?
(eσ2e−1)d2e|(eσ2e+1)d2e|(eσ2e−1)/d2e|(eσ2e+1)/d2e
[0]


What is the expected value of θe in the greedy algorithm?
Ep[θe] = eµe+σ2e/2|Ep[θe] = eµe-σ2e/2|Ep[θe] = ln(µe)-σ2e/2|Ep[θe] = ln(µe)+σ2e/2
[0]


How does the TS algorithm select its action?
By randomly drawing ˆθefrom a log-Gaussian distribution with parameters µeandσ2e|By setting ˆθeto the expected valueEp[θe]|By minimizing the variance of mean travel time along an edge|By maximizing the expected value of θe
[0]


What problem-solving algorithm is used in the given scenario?
A. Breadth-first search|B. Depth-first search|C. Dijkstra's algorithm|D. A* algorithm
[2]


What is the shape of the graph in Example 4.1?
A. A circle|B. A binomial bridge|C. A straight line|D. A parabola
[1]


What are the prior parameters set for each e∈E?
A. µe=1 and σ2e=−12|B. µe=−12 and σ2e=1|C. µe=0 and σ2e=1|D. µe=1 and σ2e=0
[1]


What is the purpose of /epsilon1-greedy exploration?
A. To traverse paths produced by a greedy algorithm|B. To sample a path randomly|C. To explore informative paths|D. To explore entirely random paths
[0]


Which algorithm converges quickly to optimal in the given scenario?
A. Greedy algorithm|B. TS algorithm|C. Breadth-first search|D. Depth-first search
[1]


What is the purpose of the plots in Figure 4.1?
To compare the performance of Thompson sampling and /epsilon1-greedy algorithms in the shortest path problem|To show the distribution of log-Gaussian-distributed parameters|To demonstrate the convergence rate of Thompson sampling|To model complex information structures
[0]


What is Algorithm 4 used for?
To solve problems with complex information structures|To compare the performance of different algorithms in the shortest path problem|To demonstrate the convergence rate of Thompson sampling|To model log-Gaussian-distributed parameters
[0]


What is Example 4.2 about?
A more complex variation of the binomial bridge example|The performance of Thompson sampling and /epsilon1-greedy algorithms in the shortest path problem|The distribution of log-Gaussian-distributed parameters|The modeling of complex information structures
[0]


What is assumed about the idiosyncratic factors in Example 4.2?
They are independent and identically distributed with mean 0 and variance 1|They are log-Gaussian-distributed with parameters µe and σ2e|They are Gaussian with variance σ2y|They are correlated with the observation noise
[0]


What is the observation distribution characterized by in Example 4.2?
yt,e=ζt,eηtνt,/lscript(e)θe|yt,e=ζt,eηtνt|yt,e=ζt,e/lscript(e)θe|yt,e=ζt,eηtνt/lscript(e)θe
[0]


What is the distribution of shocks ζt,e,ηt,νt,0, and νt,1 in the model?
Log-Gaussian|Uniform|Normal|Exponential
[0]


What must be learned through experimentation in the model?
The distributions of the shocks|The common factors|The marginal distribution of yt,e|θ|The joint distribution over yt|θ
[0]


What induces correlations among travel times in the binomial bridge?
The common factors|The distributions of the shocks|The marginal distribution of yt,e|θ|The joint distribution over yt|θ
[0]


What is the role of ηt in the model?
To model the impact of random events that influence traffic conditions everywhere|To reflect events that bear influence only on traffic conditions along edges in half of the binomial bridge|To induce correlations among travel times in the binomial bridge|To learn through experimentation
[0]


What facilitates efficient updating of posterior parameters in the model?
Conjugacy properties|The distributions of the shocks|The common factors|The marginal distribution of yt,e|θ
[0]


What is the posterior distribution of φ?
Gaussian|Binomial|Poisson|Exponential
[0]


How is the posterior distribution updated?
By adding the inverse of the sum of Σ^-1 and ˜C|By subtracting the inverse of the sum of Σ^-1 and ˜C|By multiplying the sum of Σ^-1 and ˜C|By dividing the sum of Σ^-1 and ˜C
[0]


How is the sample ˆθ drawn?
By sampling a vector ˆφ from a Gaussian distribution with mean µ and covariance matrix Σ|By sampling a vector ˆφ from a Binomial distribution with mean µ and covariance matrix Σ|By sampling a vector ˆφ from a Poisson distribution with mean µ and covariance matrix Σ|By sampling a vector ˆφ from an Exponential distribution with mean µ and covariance matrix Σ
[0]


How is the selected action determined?
By maximizing Eqˆθ[r(yt)|xt=x] =−/summationtexte∈xtˆθe|By minimizing Eqˆθ[r(yt)|xt=x] =−/summationtexte∈xtˆθe|By maximizing Eqˆθ[r(yt)|xt=x] =/summationtexte∈xtˆθe|By minimizing Eqˆθ[r(yt)|xt=x] =/summationtexte∈xtˆθe
[0]


What are the four approaches to approximate posterior sampling discussed in this section?
Gibbs sampling, Langevin Monte Carlo, sampling from a Laplace approximation, and the bootstrap|Bayesian inference, computational efficiency, binary feedback, and online shortest path problem|Deterministic travel times, gamma distribution, binomial bridge, and route recommendations|Exact Bayesian inference, Laplace approximation, Gibbs sampling, and binary observations
[0, 0, 0, 0]


What is the purpose of using approximate posterior sampling methods?
To deal with problems that are not amenable to efficient Bayesian inference|To simplify the model formulation process|To generate exact posterior distributions|To improve computational efficiency
[0]


What is Example 5.1 about?
Binary feedback in a variation of the online shortest path problem|Exact Bayesian inference in a binomial bridge model|Computational efficiency in gamma-distributed models|Laplace approximation in deterministic travel time models
[0]


What is the reward in Example 5.1?
The rating of the route|The travel time|The feedback from the driver|The recommendation of the system
[0]


What is the information structure used in Example 5.1?
Binary feedback with gamma-distributed travel times|Deterministic travel times with binary observations|Binomial bridge with route recommendations|Gamma distribution with online shortest path problem
[0]


The use of approximation methods in a new model for efficient learning
Bayesian inference|Approximate TS|Langevin Monte Carlo|Binary feedback
[1]


The new model does not enjoy conjugacy properties and is not amenable to efficient exact Bayesian inference
It is not applicable to online shortest path problems|It does not provide accurate results|It is too complex to implement|It does not address the issue of feedback
[0]


Langevin Monte Carlo, the Laplace approximation, and the bootstrap
Bayesian inference|Conjugacy properties|Binary feedback|Optimal path
[0, 1, 2]


To serve as a baseline for comparison with the results from application of the approximate versions of TS
To generate a sample ˆθ|To solve for the optimal path under ˆθ|To approximate the distribution of ˆθ|To address the issue of feedback
[0]


What is Gibbs sampling?
A method for drawing approximate samples from multivariate probability distributions|A method for exact sampling from multivariate probability distributions|A method for drawing approximate samples from univariate probability distributions|A method for exact sampling from univariate probability distributions
[0]


What is the initial guess in Gibbs sampling?
ˆθ0|ft−1|fn,kt−1|ˆθN
[0]


What is the purpose of iterating over sweeps in Gibbs sampling?
To generate a sequence of sampled parameters forming a Markov chain with stationary distribution ft−1|To generate a sequence of sampled parameters forming a Markov chain with non-stationary distribution ft−1|To generate a sequence of sampled parameters forming a Markov chain with stationary distribution fn,kt−1|To generate a sequence of sampled parameters forming a Markov chain with non-stationary distribution fn,kt−1
[0]


What is the marginal distribution in Gibbs sampling?
fn,kt−1(θk)|ft−1(ˆθn1,..., ˆθnk−1,θk,ˆθn−1k+1,..., ˆθn−1K)|ˆθnk|ˆθN
[0, 1]


What is the prevailing vector in Gibbs sampling?
ˆθN|ˆθ0|ft−1|fn,kt−1
[0]


What is Gibbs sampling?
A method for approximating a posterior distribution by a Gaussian distribution|A method for sampling from a one-dimensional distribution|A method for approximating a posterior distribution by a unimodal distribution|A method for approximating a posterior distribution by a bimodal distribution
[1]


Why is Gibbs sampling computationally viable?
Because it applies to a broad range of problems|Because it is easy to implement|Because it generates a close approximation to a posterior sample within well under a minute|Because it is computationally demanding
[0]


What is Laplace Approximation?
An approach that approximates a potentially complicated posterior distribution by a Gaussian distribution|An approach that approximates a posterior distribution by a unimodal distribution|An approach that approximates a posterior distribution by a bimodal distribution|An approach that approximates a posterior distribution by a Poisson distribution
[0]


When is it natural to consider approximating a probability density function locally around its mode?
When the log density of the function is strictly concave around its mode|When the log density of the function is strictly convex around its mode|When the log density of the function is linear around its mode|When the log density of the function is constant around its mode
[0]


What is the Laplace approximation of g?
A. A density proportional to a Gaussian distribution with mean φ and covariance C-1|B. A density proportional to a uniform distribution with mean φ and variance C-1|C. A density proportional to a Poisson distribution with mean φ and variance C-1|D. A density proportional to a Bernoulli distribution with mean φ and variance C-1
[0]


What is the advantage of using the Laplace approximation to sample from g?
A. It allows for exact sampling from g|B. It is computationally efficient|C. It guarantees convergence to the true distribution|D. It is more accurate than other sampling methods
[1]


How is the mode θ of the posterior density ft-1 efficiently computed?
A. By drawing a sample from a Gaussian distribution with mean θ and covariance matrix C|B. By maximizing ft-1, which is log-concave|C. By minimizing ft-1, which is log-convex|D. By drawing a sample from a uniform distribution with mean θ and variance C
[1]


What is Laplace approximation?
A method to generate multiple choice questions|A method to approximate posterior distributions with smooth densities|A method to compute the Hessian of the log-posterior density|A method to substitute variables
[1]


What type of posterior distributions are well suited for Laplace approximations?
Posterior distributions with smooth densities that are sharply peaked around their mode|Posterior distributions with non-smooth densities|Posterior distributions with uniform densities|Posterior distributions with multimodal densities
[0]


When are Laplace approximations computationally efficient?
When one can efficiently compute the posterior mode and form the Hessian of the log-posterior density|When one can efficiently compute the posterior mean and variance|When one can efficiently compute the posterior median and mode|When one can efficiently compute the posterior standard deviation and skewness
[0]


Is the behavior of Laplace approximation invariant to a substitution of variables?
No|Yes
[0]


How can a substitution of variables enhance the efficacy of Laplace approximation?
By making the Laplace approximation exact|By making the Laplace approximation more approximate|By making the Laplace approximation faster|By making the Laplace approximation slower
[0]


What is the Laplace approximation?
A method to maximize a log-concave probability density function|A Markov chain Monte Carlo method|A backtracking line search algorithm|A standard Brownian motion process
[0]


What method was used to maximize 5.3 in Figure 5.1?
Newton's method with a backtracking line search|Langevin Monte Carlo|The Laplace approximation|Markov chain Monte Carlo
[0]


Why does the Laplace approximation fall short of Langevin Monte Carlo in the example discussed?
The posterior distribution is not sufficiently close to Gaussian|The Laplace approximation is not a log-concave probability density function|Langevin Monte Carlo is not a Markov chain Monte Carlo method|The Laplace approximation is not diﬀerentiable
[0]


What is Langevin dynamics?
A diﬀusion process that uses gradient information about the target distribution|A backtracking line search algorithm|A log-concave probability density function|A standard Brownian motion process
[0]


What is the unique stationary distribution of Langevin dynamics?
The log-concave probability density function g(φ)|A standard Brownian motion process|The posterior distribution|The Laplace approximation
[0]


What is the purpose of equation (5.2)?
To implement a Euler discretization of a stochastic differential equation|To establish rigorous guarantees for the rate of convergence of a Markov chain|To inject random Gaussian noise at each step|To compute approximate gradients using minibatches of data
[0]


What is the effect of injecting random Gaussian noise in equation (5.2)?
To make the position of φn random and capture the uncertainty in the distribution g|To make the position of φn deterministic and capture the certainty in the distribution g|To decrease the rate of convergence of the Markov chain|To increase the computational efficiency of the method
[0]


What is stochastic gradient Langevin Monte Carlo?
A method that uses sampled minibatches of data to compute approximate gradients|A method that establishes rigorous guarantees for the rate of convergence of a Markov chain|A method that injects random Gaussian noise at each step|A method that implements a Euler discretization of a stochastic differential equation
[0]


What is the mini-batch size used in the implementation of stochastic gradient Langevin Monte Carlo?
100|10|1000|50
[0]


What is the purpose of the two standard modifications made to the method?
To improve computational efficiency|To decrease the rate of convergence of the Markov chain|To increase the accuracy of the method|To make the position of φn deterministic and capture the certainty in the distribution g
[0]


What is the purpose of using an estimated gradient in the Markov chain when more than 100 observations have been gathered?
To improve the mixing rate of the Markov chain|To reduce the impact of noise in gradient estimation|To speed up the convergence of gradient ascent|To compute the posterior mode
[1]


What is the second modification made to the Markov chain in Example 5.1?
The use of a preconditioning matrix|The addition of Gaussian noise|The estimation of the gradient|The initialization of the chain at the posterior mode
[0]


What is the benefit of using a preconditioning matrix in Langevin MCMC?
It improves the mixing rate of the Markov chain|It reduces the impact of noise in gradient estimation|It speeds up the convergence of gradient ascent|It computes the posterior mode
[0]


How is Langevin MCMC implemented with a preconditioning matrix?
By simulating the Markov chain with a scaled gradient and a random noise term|By computing the posterior mode via means discussed in Section 5.2|By initializing the chain at the posterior mode|By adding a Gaussian noise term to the gradient
[0]


What is the posterior mode?
The maximum value of the posterior distribution|The minimum value of the posterior distribution|The average value of the posterior distribution|The median value of the posterior distribution
[0]


What is the preconditioning matrix A used in the Laplace approximation approach?
The negative inverse Hessian at the posterior mode|The gradient of the posterior distribution at the posterior mode|The Hessian of the posterior distribution at the posterior mode|The inverse of the posterior distribution at the posterior mode
[0]


What is the bootstrap method used for in this context?
Approximately sampling from a posterior distribution|Evaluating the maximum likelihood estimate of theta|Generating a hypothetical history of action-observation pairs|Maximizing the likelihood of theta under the hypothetical history
[0]


What is the hypothetical history generated in the bootstrap method?
A sequence of action-observation pairs sampled uniformly with replacement from the original history|A sequence of action-observation pairs sampled without replacement from the original history|A sequence of action-observation pairs sampled uniformly with replacement from a different history|A sequence of action-observation pairs sampled without replacement from a different history
[0]


What is the likelihood function used in the bootstrap method for the shortest path recommendation problem?
The likelihood of theta under the hypothetical history|The likelihood of the hypothetical history under theta|The posterior distribution of theta given the hypothetical history|The posterior distribution of the hypothetical history given theta
[0]


What is the hypothetical history used for in the shortest path recommendation problem?
To estimate the maximum likelihood estimate|To take the agent's prior into account|To draw a sample from the prior distribution|To solve the maximization problem
[0]


What is the issue with using the hypothetical history method in initial periods?
It overestimates the agent's real uncertainty|It underestimates the agent's real uncertainty|It does not take the agent's prior into account|It does not draw a sample from the prior distribution
[1]


What is the purpose of the modification described in the text?
To estimate the maximum likelihood estimate|To take the agent's prior into account|To draw a sample from the prior distribution|To solve the maximization problem
[1]


What is the final step in the modified method described in the text?
Drawing a hypothetical history|Drawing a sample from the prior distribution|Solving the maximization problem|Calculating the covariance matrix of the prior
[2]


What is the maximization problem in the given context?
Maximize ˆθ|Maximize ˆft−1|Maximize θ0|Maximize Σ
[1]


What is the purpose of treating ˆθ as an approximate posterior sample?
To view it as a randomized approximation to the posterior density|To view it as a deterministic approximation to the posterior density|To view it as a prior sample|To view it as a likelihood sample
[0]


What is the effect of the random prior sample θ0 on the agent's behavior?
It encourages the agent to explore in early periods|It encourages the agent to exploit in early periods|It encourages the agent to explore in later periods|It encourages the agent to exploit in later periods
[0]


What is the likelihood function in the context of the shortest path recommendation problem?
It is log-concave|It is log-convex|It is linear|It is quadratic
[0]


What optimization method was used to maximize ln(ˆft−1) in the given context?
Newton's method with a backtracking line search|Gradient descent|Random search|Simulated annealing
[0]


What can be done if it is not possible to efficiently maximize ˆft−1?
The bootstrap approach can be applied with heuristic optimization methods that identify local or approximate maxima|The bootstrap approach cannot be applied|The agent should stop exploring|The agent should only exploit
[0]


What is the advantage of using bootstrap approach over Laplace approximation and Langevin Monte Carlo?
It is nonparametric and may work regardless of the functional form of the posterior distribution|It relies on log-concavity and other regularity assumptions|It is guaranteed to perform well for all problem classes|None of the above
[0]


What is the lack of theoretical justification for bootstrap approaches?
There is no understanding of whether there are nontrivial problem classes for which they are guaranteed to perform well|They rely on log-concavity and other regularity assumptions|They are not nonparametric|None of the above
[0]


What does Figure 5.1 demonstrate about Laplace approximation, Langevin Monte Carlo, and bootstrap approaches?
They learn from binary feedback to improve performance over time|They do not work well for the path recommendation problem|They rely on exact TS for performance improvement|None of the above
[0]


What problem is used for comparison between performance of exact and approximate methods?
Three-armed beta-Bernoulli bandit problem|Path recommendation problem|Gibbs sampling problem|None of the above
[0]


What is the method used for exact posterior samples in the three-armed beta-Bernoulli bandit problem?
Gibbs sampling|Laplace approximation|Bootstrap approach|None of the above
[0]


What is the topic of the text?
Approximation methods in decision making|Exact Thompson sampling algorithms|Online shortest path problem|Performance evaluation of different algorithms
[0]


Which approximation method performs marginally worse than alternatives in the first problem discussed?
Laplace approximation|Gibbs sampling|Langevin Monte Carlo|Bootstrap
[0]


What is the purpose of the simulations in the online shortest path problem?
To optimize results for the specific problem|To offer a sanity check for the approach|To compare different approximation methods|To demonstrate the effectiveness of exact Thompson sampling
[1]


How does the computation time required per time period differ between approximation methods and exact Thompson sampling algorithms?
It grows as time progresses for approximation methods, but not for exact Thompson sampling algorithms|It remains constant for both approximation methods and exact Thompson sampling algorithms|It decreases as time progresses for approximation methods, but not for exact Thompson sampling algorithms|It is not mentioned in the text
[0]


What is the purpose of incremental variants of approximate posterior sampling algorithms?
To maintain parameters that encode a posterior distribution and update these parameters over each time period based only on the most recent observation|To increase the computational burden of the algorithm|To consider growing per-period compute time|To design exact posterior sampling algorithms
[0]


What is an incremental algorithm?
An algorithm that operates with fixed rather than growing per-period compute time|An algorithm that operates with growing per-period compute time|An algorithm that updates parameters based on all previous observations|An algorithm that updates parameters based only on the most recent observation
[0]


What is the purpose of Laplace approximation in the incremental algorithm?
To identify the mode of ft-1|To compute the posterior density at each time t|To update the statistics Ht and θt|To initialize the algorithm with θ0
[0]


How is the Laplace approximation algorithm updated incrementally?
Ht=Ht-1+∇2gt(θt-1), θt=θt-1-H-1t∇gt(θt-1)|Ht=Ht-1-∇2gt(θt-1), θt=θt-1+H-1t∇gt(θt-1)|Ht=Ht-1+∇2gt(θt), θt=θt-1-H-1t∇gt(θt)|Ht=Ht-1-∇2gt(θt), θt=θt-1+H-1t∇gt(θt)
[0]


What is ensemble sampling?
A method of randomly drawing from a range of statistically plausible models|A method of fitting each model to a different bootstrap sample|A method of maintaining, incrementally updating, and sampling from a finite set of models|A method of exploring deep reinforcement learning
[2]


How does ensemble sampling approximate the posterior distribution?
By randomly selecting models from a range of statistically plausible models|By maintaining, incrementally updating, and sampling from a finite set of models|By fitting each model to a different bootstrap sample|By exploring deep reinforcement learning
[1]


What is the bootstrap method in ensemble sampling?
A method of randomly drawing from a range of statistically plausible models|A method of fitting each model to a different bootstrap sample|A method of maintaining, incrementally updating, and sampling from a finite set of models|A method of exploring deep reinforcement learning
[1]


What is the role of the variable znt in ensemble sampling?
To initialize each set of models with parameters|To update each set of models according to a Poisson distribution|To randomly weight each observation in the model|To approximate the posterior distribution
[2]


What is the variable znτ in the given text?
A. A hypothetical history of the data set|B. A number of replicas of the data sample|C. A distribution of the data sample|D. A posterior sample of the data set
[1]


What distribution does the number of replicas of a particular bootstrap data sample follow?
A. Poisson (t,1/t)|B. Binomial (t,1/t)|C. Normal distribution|D. Uniform distribution
[0]


What is the purpose of generating an action xt,n in the given text?
A. To fit the data sample to a different dataset|B. To choose the action that maximizes the expected reward|C. To update the ensemble of models|D. To apply active learning with neural networks
[1]


What is an assumption made in the idealized view of TS discussed in the given text?
A. The system and set of feasible actions is constant over time|B. There is side information on decision context|C. The process of prior specification is considered|D. The algorithms do not require a prior distribution
[0]


What is discussed in Section 7.4 of the given text?
A. The process of prior specification|B. Extensions of TS that serve practical needs|C. The purpose of generating an action xt,n|D. The number of replicas of a particular bootstrap data sample
[1]


What is the role of prior distribution in the presented algorithms?
To optimize performance for a single value|To perform well on average across a collection of possibilities|To ignore any useful knowledge from past experience|To reduce the time it takes for TS to identify the most effective ads
[1]


What can the prior be thought of in the context of designing an algorithm for an online decision problem?
A distribution over plausible values|A fixed value for θ|A uniform distribution over each θk|A beta distribution over each θk
[0]


What is the banner ad placement problem and how can prior selection improve its performance?
The problem of selecting the most successful ad given unknown click-through probabilities|Using a uniform prior to represent no understanding of the context|Reducing what must be learned and therefore reducing the time it takes for TS to identify the most effective ads|Partitioning past ads into K
[2]


What is the empirical approach to prior selection discussed in the text?
Leveraging data from past experience to partition ads into K|Using a uniform prior to represent no understanding of the context|Optimizing performance for a single value|Ignoring any useful knowledge from past experience
[0]


What is the purpose of the three curves in Figure 6.2?
To illustrate how starting with a misspecified prior delays learning|To show the difference between coherent and misspecified priors|To compare the best, second-best, and worst actions|To demonstrate the effectiveness of TS
[0]


What is one way to extend Algorithm 4 to accommodate time-varying constraints on actions?
By imposing a sequence of admissible action sets Xt|By modifying the maximization problem in Line 6|By changing the prior distribution p(θ)|By adding a new parameter to the model
[1]


What is a contextual online decision problem?
A problem where the response to an action depends on an independent random variable observed prior to making the decision|A problem where the agent must make a decision without any prior information|A problem where the agent must make a decision based on a fixed set of options|A problem where the agent must make a decision based on a known prior distribution
[0]


To discuss how to address contextual problems in decision making
To provide a history of nonstationary systems|To explain how to use TS in stationary systems|To discuss the importance of expected performance
[0]


To address contextual problems in decision making
To make the agent's actions more random|To limit the agent's choices|To ensure the agent always chooses the optimal action|To make the agent's actions more predictable
[0, 2]


By allowing the agent to dictate both the weather report and the path to traverse, but constraining the agent to provide a weather report identical to the one observed through the news channel
By allowing the agent to choose any path they want|By only allowing the agent to choose the path observed through the news channel|By only allowing the agent to choose the weather report|By not allowing the agent to choose anything
[0]


To ensure a level of caution against poor performance
To make the agent's actions more random|To limit the agent's choices|To ensure the agent always chooses the optimal action|To make the agent's actions more predictable
[0]


A system where model parameters are not constant over time
A system where the agent's actions are always random|A system where the agent's choices are always limited|A system where the agent always chooses the optimal action|A system where the agent's actions are always predictable
[0]


What is the problem with using a stationary model in a nonstationary system?
It is not appropriate to model time-varying parameters with a stationary model|It is not possible to model time-varying parameters with a stationary model|It is too complicated to model time-varying parameters with a stationary model|None of the above
[0]


Why should an agent never stop exploring in a nonstationary system?
To track changes as the system drifts|To avoid making mistakes|To save computational resources|None of the above
[0]


What is one simple approach to addressing nonstationarity?
Ignoring historical observations beyond a certain number of time periods|Using a stationary model|Stopping exploration after a certain number of time periods|None of the above
[0]


How does the alternative approach to addressing nonstationarity model the evolution of a belief distribution?
In a manner that discounts the relevance of past observations and tracks a time-varying parameter θt|By using a stationary model|By ignoring historical observations beyond a certain number of time periods|None of the above
[0]


What is a suitable modification of TS for the alternative approach to addressing nonstationarity?
Modeling the evolution of a belief distribution that tracks a time-varying parameter θt|Ignoring historical observations beyond a certain number of time periods|Using a stationary model|None of the above
[0]


What is the prior distribution for each kth mean reward in the Bernoulli bandit problem?
beta(α,β)|gamma(α,β)|normal(μ,σ)|exponential(λ)
[0]


What does the algorithm update in the Bernoulli bandit problem?
parameters to identify the belief distribution of θt conditioned on the history Ht−1|parameters to identify the belief distribution of θt conditioned on the future Ht+1|parameters to identify the belief distribution of θt conditioned on the present Ht|parameters to identify the belief distribution of θt conditioned on all observations
[0]


What does the parameter γ control in the Bernoulli bandit problem?
how quickly uncertainty is injected|how quickly the algorithm converges|how quickly the mean reward changes|how quickly the variance changes
[0]


What happens to the belief distribution in the absence of observations in the Bernoulli bandit problem?
it converges to beta(αk,βk)|it converges to gamma(α,β)|it converges to normal(μ,σ)|it converges to exponential(λ)
[0]


What happens to the parameters (αk,βk) in the absence of observations if γ > 0 in the Bernoulli bandit problem?
it converges to (αk,βk)|it converges to (α,β)|it converges to (0,0)|it converges to (1,1)
[0]


What does the parameter γ represent in the Bernoulli bandit problem?
the amount of uncertainty injected into the process|the mean reward of the bandit problem|the variance of the bandit problem|the number of arms in the bandit problem
[0]


What is the intuition behind the process in the Bernoulli bandit problem?
randomly perturbing model parameters in each time period, injecting uncertainty|randomly selecting arms in each time period, injecting uncertainty|randomly selecting rewards in each time period, injecting uncertainty|randomly selecting parameters in each time period, injecting uncertainty
[0]


What is the modified version of Algorithm 2 used for in the Bernoulli bandit problem?
to solve the nonstationary Bernoulli bandit problem|to solve the stationary Bernoulli bandit problem|to solve the multi-armed bandit problem|to solve the contextual bandit problem
[0]


p(u)←p(u)qu(yt|xt)/summationtextvp(v)qv(yt|xt).
p(u)qu(yt|xt)/summationtextvp(v)qv(yt|xt)|p(u)qv(yt|xt)/summationtextvp(v)qu(yt|xt)|p(v)qu(yt|xt)/summationtextup(u)p(u)qv(yt|xt)|p(v)qv(yt|xt)/summationtextup(u)p(u)qu(yt|xt)
[0]


By using the formula p(u)←pγ(u)p1−γ(u)qu(yt|xt)/summationtextvpγ(v)p1−γ(v)qv(yt|xt).
p(u)qu(yt|xt)/summationtextvp(v)qv(yt|xt)|p(u)p1−γ(u)qu(yt|xt)/summationtextvp(v)pγ(v)qv(yt|xt)|p(v)p1−γ(v)qv(yt|xt)/summationtextup(u)p(u)pγ(u)qu(yt|xt)|p(v)qv(yt|xt)/summationtextup(u)p(u)pγ(u)p1−γ(u)qu(yt|xt)
[0]


Nonstationary TS
Stationary TS|Modified TS|Dynamic TS|Adaptive TS
[0]


The rate at which uncertainty is injected.
The rate at which the distribution parameters are updated.|The rate at which the model parameters change.|The rate at which the prior distribution changes.|The rate at which the posterior distribution changes.
[0]


What is the main focus of the text?
A. The comparison of TS and nonstationary TS algorithms|B. The application of TS in concurrent actions|C. The limitations of TS algorithm|D. The exploration of online shortest path problem
[1]


What is the online shortest path problem?
A. A problem where an agent selects and traverses a path from origin to destination|B. A problem where multiple agents travel between the same origin and destination|C. A problem where agents update a posterior distribution based on observed edge traversal times|D. A problem where agents draw multiple independent samples to produce paths
[0]


What is the advantage of using TS in concurrent actions?
A. It diversifies experience|B. It improves the performance of UCB algorithm|C. It reduces the number of agents needed|D. It simplifies the posterior distribution
[0]


What does Figure 6.4(a) demonstrate?
A. TS outperforms UCB and ε-greedy algorithms in concurrent setting|B. TS is less effective than UCB and ε-greedy algorithms in concurrent setting|C. The performance gap between TS and UCB algorithms is negligible|D. The performance of TS is not affected by the number of agents
[0]


What does Figure 6.4(b) show?
A. The performance of TS improves as the number of agents increases|B. The performance of TS deteriorates as the number of agents increases|C. The performance of TS is not affected by the number of agents|D. The performance of TS is worse than UCB and ε-greedy algorithms as the number of agents increases
[0]


The performance of concurrent Thompson sampling
Web services|Concurrence|Shortest path problem|Observations
[1]


Why per-action regret decays more rapidly with time as the number of agents grows
Each agent's learning is accelerated by shared observations|The posterior distribution is updated after K concurrent actions are completed|Actions are informed by observations generated by concurrent ones|The per-action regret is not affected by the number of agents
[0]


Why per-action regret decays more slowly as a function of the number of actions taken so far by the collective of agents
The posterior distribution is updated only after K concurrent actions are completed|Actions are not informed by observations generated by concurrent ones|Each agent's learning is not accelerated by shared observations|The per-action regret is not affected by the number of actions
[1]


The role of concurrence in web services
Providing different versions of a service to different users|Selecting actions on demand|Updating the posterior distribution as data becomes available|Altering an action based on recent experience of other agents
[0]


The difference between synchronous and asynchronous variations of concurrent Thompson sampling
Synchronous variation involves action selection and posterior updating, while asynchronous variation does not|Asynchronous variation involves action selection and posterior updating, while synchronous variation does not|Synchronous variation is more efficient than asynchronous variation|Asynchronous variation is more efficient than synchronous variation
[1]


What is the challenge in designing a version of TS that adapts to new observations while still exploring efficiently?
To avoid edges with long expected travel time|To explore in a reliable and efficient manner|To adapt to new observations|To design a version of TS
[0, 1, 2]


